---
published: false
---
Récemment j'ai commencé à m'intéresser au sujet du traitement automatique du langage naturel (natural language processing ou NLP en anglais) et je veux comprendre la base de cette nouvelle technologie qui me semble se met au jour extrêmement rapidement. L'un des sujets sur lequel j'ai lu est un mécanisme qui est bien nommé Attention.

Il s'agit d'une relativement nouvelle technique qui était inventée en 2016 et qui est souvent utilisé maintenant pour les tâches de traduction. L'idée générale n'est pas trop énigmatique : après avoir appris un grand nombre d'exemples, un réseau neuronal artificiel comprend comment faire attention à un ou plusieurs mots spécifiques de l'entrée chaque fois qu'il génère un mot de la traduction. Par exemple, s'il devait traduire « I went to the park » en français, pour le premier mot de la traduction, il ferait le plus d'attention au mot « I » et il donc créerait le mot « I » ; mais pour les mots suivants de la traduction (« suis » et « allé »), il ferait attention à tous les deux « I » et « went » parce qu'ils sont tous importants pour déterminer le verbe et sa conjugaison correcte en français. Grâce à cette sensibilité au contexte, il est capable de traduire plus précisément et de manière plus flexible. Bien sûr, c'est une explication simpliste et si vous êtes spécialiste dans ce domaine, n'hésitez pas à me corriger si j'ai fait des erreurs !

Cela a vraiment du sens en fait, parce que je constate que des traducteurs automatiques, en particulier Google Translate, sont devenus beaucoup mieux après 2016 environ. En effet, Google utilise le mécanisme Attention pour ce produit.