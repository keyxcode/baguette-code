---
published: true
---
Récemment j'ai commencé à m'intéresser au sujet du traitement automatique du langage naturel (Natural language processing ou NLP en anglais) et je veux comprendre la base de cette technologie qui semble se met au jour extrêmement rapidement. L'un des sujets sur lequel j'ai lu est un mécanisme au nom approprié, « Attention ».

Il s'agit d'une technique relativement nouvelle qui a été inventée en 2016 et qui est désormais souvent utilisé pour les travaux de traduction. L'idée générale n'est pas trop obscure : après avoir appris un grand nombre d'exemples, un réseau neuronal artificiel comprend comment faire plus d'attention à un ou plusieurs mots spécifiques d'une entrée chaque fois qu'il génère un mot de la traduction. Par exemple, s'il doit traduire « I went to the park » en français, pour le premier mot de la traduction, il se concentrera surtout sur le mot « I » et il va donc donner le mot « Je », mais pour les mots suivants de la traduction (« suis » et « allé »), il va faire plus attention à « I » et « went » parce qu'ils sont tous les deux importants pour déterminer le verbe et sa conjugaison correcte en français. Grâce à cette sensibilité au contexte, il est capable de traduire plus précisément et de manière plus flexible. Bien sûr, c'est une explication simpliste et si vous êtes spécialiste de ce domaine, n'hésitez pas à me corriger si j'ai fait des erreurs !

Cela a vraiment du sens en fait, parce que je constate que les traducteurs automatiques, en particulier Google Translate, sont devenus bien meilleurs après 2016 environ. En effet, Google utilise le mécanisme « Attention » pour ce produit.
