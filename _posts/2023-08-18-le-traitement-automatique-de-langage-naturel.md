---
published: false
---
Récemment j'ai commencé à  m'intéresser au sujet du traitement automatique de langage naturel (natural language processing ou NLP en anglais) et je veux comprendre sa base. L'un des sujets sur lequel j'ai lu est un mécanisme intéressant qui est bien nommé Attention.

C'est une relativement nouvelle technique qui était inventée en 2016 et qui est souvent utilisé maintenant pour des tâches de traduction. L'idée générale n'est pas trop énigmatique : après avoir appris un grand nombre d'exemples, un réseau neuronal comprend comment faire attention à un ou plusieurs mots spécifiques de l'entrée chaque fois qu'il produit un mot de la traduction. Par exemple, s'il doit traduire « I went to the park » en français, pour le premier mot qu'il produit (« Je »), il fait le plus d'attention au mot « I », mais pour les prochains mots (« suis » et « allé »), il fait attention à les deux « I » et « went » parce qu'ils sont tous les deux importants pour déterminer le verbe et sa conjugaison en français. Grâce à cette sensibilité au contexte, il est capable de traduire beaucoup plus précisement. Bien sûr, c'est une explication très simpliste et si vous êtes spécialiste dans ce domaine, n'hésitez pas à me corrigez si j'ai fait des eurreurs !

Ça fait du sens en fait, parce que je constate que des traducteurs automatiques, particulièrement Google Translate, sont devenus beaucoup mieux après 2016 environ. En effet, Google utilise le mécanisme Attention pour ce produit. Je crois que c'est important d'avoir une connaissance de base de cette nouvelle technologie qui se met au jour extrêmement rapidement.